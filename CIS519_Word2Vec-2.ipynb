{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "1ekLfZo0FsEa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2-2h8LK9-6"
      },
      "source": [
        "# Accessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "_AsVd1HjJziu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdeadc4e-211d-4747-b4f8-dd38ef41172b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "U1B1x9aPKIKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f714fa94-7ac0-4a00-8751-f17f5d392043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                               text  label\n",
            "0           0        im sick with allergies and feeling horrible      0\n",
            "1           1  i feel the music hit me in a vain attempt to k...      0\n",
            "2           2  i feel terribly helpless and thus i am putting...      0\n",
            "3           3  im feeling like ive missed you all this time s...      0\n",
            "4           4  im finding it harder and harder every day to c...      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/Shareddrives/CIS 5190 Final Project!/new_emotions_df.csv')\n",
        "print(df.head()) #Check display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGtkvxXJLEgX"
      },
      "source": [
        "# Cleaning Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "NAyCnOLGKrlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec350ad2-6cf5-41cf-d200-6f9ff6e267cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9000\n",
            "9000\n"
          ]
        }
      ],
      "source": [
        "# Check for NA values and drop them\n",
        "print(len(df)) #9000\n",
        "df.dropna(inplace = True)\n",
        "print(len(df)) #9000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample the data for all vectorization techniques to reduce run time burden on transformers\n",
        "# There is no perfromance difference between a sample of 7k and the full data set (9k), so start there\n",
        "data_sample = df.sample(n = 7000, random_state = 19104, ignore_index = True)\n",
        "print(data_sample.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHPYNmXWM0cC",
        "outputId": "8a28762c-d080-4a29-eb7b-975593a72970"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the x and y variables\n",
        "texts = data_sample['text']\n",
        "labels = data_sample['label']"
      ],
      "metadata": {
        "id": "8P-tnmFkPH-k"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import pretrained word vectors\n"
      ],
      "metadata": {
        "id": "1dntkEcSUGGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "26ScDojGUNug"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-trained Word2Vec embeddings\n",
        "embedding_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "nDh9vZ7ejOh4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare text for word2vec embedding on svm and logistic regression\n"
      ],
      "metadata": {
        "id": "K4sIQJQXSYT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a text length distribution to decide on an optimal max_len\n",
        "text_lengths = [len(text.split()) for text in texts] #list of lengths\n",
        "print(\"Average length: \" , np.mean(text_lengths)) #average text length\n",
        "print(\"Max length: \", np.max(text_lengths)) #highest text length\n",
        "print(\"95% percentile: \" , np.percentile(text_lengths, 95)) #95th percentile length\n",
        "\n",
        "#Choose the max_len based on the 95th percentile (will catch most words)\n",
        "max_len = int(np.percentile(text_lengths, 95))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2bGwv-eMOUC",
        "outputId": "2f20ba15-5485-4414-9d99-221413400248"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length:  46.898\n",
            "Max length:  1400\n",
            "95% percentile:  169.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the text by tokenizing, embedding, and vectorizing"
      ],
      "metadata": {
        "id": "BEzJRESxSg_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xoVd-6Qc_LSV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess texts\n",
        "def preprocess_text(text):\n",
        "  stop_words = set(stopwords.words('english')) #create an array of common English stop words\n",
        "  words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words] #only store non-stop word text\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words] #lemmatize words: improved accuracy by ~5%\n",
        "  return ' '.join(lemmatized_words)\n",
        "\n",
        "texts = texts.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "furs68cL_QQM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and sequence padding\n",
        "tokenizer = Tokenizer(num_words = 5000)\n",
        "tokenizer.fit_on_texts(texts) # fit tokenizer on text\n",
        "sequences = tokenizer.texts_to_sequences(texts) #convert texts to sequences of integers\n",
        "word_index = tokenizer.word_index #create a word index that maps words to their indices\n",
        "max_len = 170  # set based on code above (data distribution)\n",
        "data = pad_sequences(sequences, maxlen=max_len) #pad sequences to ensre the length of the text os uniform"
      ],
      "metadata": {
        "id": "yb3Bv0hI_W1C"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim)) #initialize embedding with shape: (number of words )\n",
        "for word, i in word_index.items(): #for each word_index dictionary\n",
        "  if word in embedding_model: #if word exists in the embedding model\n",
        "    embedding_vector = embedding_model[word] #retrieve the embedding vector and add it to the corresponding row in the embedding matrix\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "7pQD0GGa_ZGT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create feature vectors by averaging word vectors\n",
        "def create_feature_vectors(data, embeddig_matrix):\n",
        "  features = np.zeros((data.shape[0], embedding_matrix.shape[1])) #features shape: (number of sequences, embedding dimension)\n",
        "  for i, sequence in enumerate(data): #for each sequence in the data\n",
        "    valid_embeddings = [embedding_matrix[word] for word in sequence if word > 0] #extract the embeddings for words in the sequence\n",
        "    #  that have embeddings (word > 0)\n",
        "    if valid_embeddings:\n",
        "      features[i] = np.max(valid_embeddings, axis = 0) #take the max. valid embedding along each dimension to create the feature vector\n",
        "    else:\n",
        "      features[i] = np.zeros(embedding_matrix.shape[1]) #if there are no feature vectors, feature vector remains a zero vector\n",
        "  return features\n",
        "\n",
        "features = create_feature_vectors(data, embedding_matrix)"
      ],
      "metadata": {
        "id": "AKlE0DYl_cMe"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_index) +1)\n",
        "print(len(embedding_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcDSJqAeDJzJ",
        "outputId": "20468440-8266-427b-a532-861090e1d849"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11902\n",
            "11902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Logistic Regression and SVM Classifiers"
      ],
      "metadata": {
        "id": "jUFTB1SsIVW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "gk09ufSO_fB9"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#Automatically adjusts weights (inversly proprotional to class frequencies) to balance classes\n",
        "weights = compute_class_weight('balanced', classes = np.unique(y_train), y = y_train)\n",
        "class_weights = dict(enumerate(weights))"
      ],
      "metadata": {
        "id": "Kehkw4z8_iK_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run Logistic Regression Model\n",
        "log_reg = LogisticRegression(max_iter = 1000) #stop at 1000 iterations regardless\n",
        "log_reg.fit(x_train, y_train)\n",
        "log_reg_pred = log_reg.predict(x_test)\n",
        "log_reg_accuracy = accuracy_score(y_test, log_reg_pred) * 100 #accuracy score\n",
        "log_reg_f1 = f1_score(y_test, log_reg_pred, average = 'weighted') * 100 #f1 score\n",
        "print(\"Logistic Regression Accuracy: \", log_reg_accuracy)\n",
        "print(\"Logistic Regression F1: \", log_reg_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuWLF3gq_lDX",
        "outputId": "c886398b-9b02-4cce-db83-37a2e529d589"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy:  67.57142857142857\n",
            "Logistic Regression F1:  67.60245254288176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run SVM Model\n",
        "svm_model = SVC()\n",
        "svm_model.fit(x_train, y_train)\n",
        "svm_pred = svm_model.predict(x_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred) * 100 #accuracy score\n",
        "svm_f1 = f1_score(y_test, svm_pred, average = 'weighted') * 100 #f1 score\n",
        "print(\"SVM Accuracy: \", svm_accuracy)\n",
        "print(\"SVM F1: \", svm_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3TBuBtO_nRy",
        "outputId": "3943b9cc-4cf8-4fc6-e334-41231f048808"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy:  70.33333333333334\n",
            "SVM F1:  70.37264658963836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9Qgs8vbClun9"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}